<!DOCTYPE html>
<html>
  <head>
    <title>Day 02 Kubernetes Fundamentals </title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
    <link rel="stylesheet" href="workshop.css">
  </head>
  <body>
    <!--
    <div style="position: absolute; left: 20%; right: 20%; top: 30%;">
      <h1 style="font-size: 3em;">Loading ...</h1>
      The slides should show up here. If they don't, it might be
      because you are accessing this file directly from your filesystem.
      It needs to be served from a web server. You can try this:
      <pre>
        docker-compose up -d
        open http://localhost:8888/workshop.html # on MacOS
        xdg-open http://localhost:8888/workshop.html # on Linux
      </pre>
      Once the slides are loaded, this notice disappears when you
      go full screen (e.g. by hitting "f").
    </div>
    -->
    <textarea id="source">class: title, self-paced

Day 02<br/>Kubernetes Fundamentals<br/><br/></br>

.debug[[shared/title.md](file:///home/xeon/urgent/slides/shared/title.md)]
---

class: title, in-person

Day 02<br/>Kubernetes Fundamentals<br/><br/></br>



<!--
WiFi: **Something**<br/>
Password: **Something**

**Be kind to the WiFi!**<br/>
*Use the 5G network.*
*Don't use your hotspot.*<br/>
*Don't stream videos or download big files during the workshop*<br/>
*Thank you!*
-->

.debug[[shared/title.md](file:///home/xeon/urgent/slides/shared/title.md)]
---

name: toc-module-0

## Table of contents

- [Kubernetes concepts](#toc-kubernetes-concepts)

- [Kubernetes Architecture](#toc-kubernetes-architecture)

- [Pods and Containers ](#toc-pods-and-containers-)

- [Setting up Kubernetes](#toc-setting-up-kubernetes)

- [Kubectl](#toc-kubectl)

- [Deployments](#toc-deployments)

- [Labels and Annotations](#toc-labels-and-annotations)

- [Volumes](#toc-volumes)

- [Configmaps | Secrets](#toc-configmaps--secrets)

.debug[(auto-generated TOC)]



.debug[[shared/toc.md](file:///home/xeon/urgent/slides/shared/toc.md)]
---

class: pic

.interstitial[![Image separating from the next module](https://gallant-turing-d0d520.netlify.com/containers/Container-Ship-Freighter-Navigation-Elbe-Romance-1782991.jpg)]

---

name: toc-kubernetes-concepts
class: title

 Kubernetes concepts

.nav[
[Previous section](#toc-)
|
[Back to table of contents](#toc-module-0)
|
[Next section](#toc-kubernetes-architecture)
]

.debug[(automatically generated title slide)]

---
# Kubernetes concepts

- Kubernetes is a container management system

- It runs and manages containerized applications on a cluster

--

- What does that really mean?

.debug[[k8s/concepts-k8s.md](file:///home/xeon/urgent/slides/k8s/concepts-k8s.md)]
---

## What can we do with Kubernetes?

- Let's imagine that we have a 3-tier e-commerce app:

  - web frontend

  - API backend

  - database (that we will keep out of Kubernetes for now)

- We have built images for our frontend and backend components

  (e.g. with Dockerfiles and `docker build`)

- We are running them successfully with a local environment

  (e.g. with Docker Compose)

- Let's see how we would deploy our app on Kubernetes!

.debug[[k8s/concepts-k8s.md](file:///home/xeon/urgent/slides/k8s/concepts-k8s.md)]
---


## Basic things we can ask Kubernetes to do

--

- Start 5 containers using image `demo/backend:v1.0`

--

- Place an internal load balancer in front of these containers

--

- Start 10 containers using image `demo/frontend:v1.3`

--

- Place a public load balancer in front of these containers

--

- It's Black Friday (or Christmas), traffic spikes, grow our cluster and add containers

--

- New release! Replace my containers with the new image `demo/frontend:v1.4`

--

- Keep processing requests during the upgrade; update my containers one at a time

.debug[[k8s/concepts-k8s.md](file:///home/xeon/urgent/slides/k8s/concepts-k8s.md)]
---

## Other things that Kubernetes can do for us

- Autoscaling

  (straightforward on CPU; more complex on other metrics)

- Resource management and scheduling

  (reserve CPU/RAM for containers; placement constraints)

- Advanced rollout patterns

  (blue/green deployment, canary deployment)

.debug[[k8s/concepts-k8s.md](file:///home/xeon/urgent/slides/k8s/concepts-k8s.md)]
---

## More things that Kubernetes can do for us

- Batch jobs

- Fine-grained access control


- Stateful services


- Automating complex tasks with *operators*


.debug[[k8s/concepts-k8s.md](file:///home/xeon/urgent/slides/k8s/concepts-k8s.md)]
---

class: pic

![that one is more like the real thing](images/cluster01.png)

.debug[[k8s/concepts-k8s.md](file:///home/xeon/urgent/slides/k8s/concepts-k8s.md)]
---

class: pic

![that one is more like the real thing](images/cluster02.png)

.debug[[k8s/concepts-k8s.md](file:///home/xeon/urgent/slides/k8s/concepts-k8s.md)]
---

class: pic

![that one is more like the real thing](images/cluster04.png)

.debug[[k8s/concepts-k8s.md](file:///home/xeon/urgent/slides/k8s/concepts-k8s.md)]
---

class: pic

![that one is more like the real thing](images/cluster05.png)

.debug[[k8s/concepts-k8s.md](file:///home/xeon/urgent/slides/k8s/concepts-k8s.md)]
---

class: pic

.interstitial[![Image separating from the next module](https://gallant-turing-d0d520.netlify.com/containers/ShippingContainerSFBay.jpg)]

---

name: toc-kubernetes-architecture
class: title

 Kubernetes Architecture

.nav[
[Previous section](#toc-kubernetes-concepts)
|
[Back to table of contents](#toc-module-0)
|
[Next section](#toc-pods-and-containers-)
]

.debug[(automatically generated title slide)]

---

class: pic

# Kubernetes Architecture

![that one is more like the real thing](images/k8s-arch2.png)


.debug[[k8s/concepts-k8s.md](file:///home/xeon/urgent/slides/k8s/concepts-k8s.md)]
---

## Kubernetes Architecture: The Worker Nodes

- The nodes executing our containers run a collection of services:

  - a container Engine (typically Docker)

  - kubelet (the "node agent")

  - kube-proxy (a necessary but not sufficient network component)

- Nodes were formerly called "minions"

  (You might see that word in older articles or documentation)

.debug[[k8s/concepts-k8s.md](file:///home/xeon/urgent/slides/k8s/concepts-k8s.md)]
---

## Kubernetes Architecture: The Control Plane


- The Kubernetes logic (its "brains") is a collection of services:

  - the API server (our point of entry to everything!)

  - core services like the scheduler and controller manager

  - `etcd` (a highly available key/value store; the "database" of Kubernetes)

- Together, these services form the control plane of our cluster

- The control plane is also called the "master"

.debug[[k8s/concepts-k8s.md](file:///home/xeon/urgent/slides/k8s/concepts-k8s.md)]
---

class: extra-details

## How many nodes should a cluster have?

- There is no particular constraint

  (no need to have an odd number of nodes for quorum)

- A cluster can have zero node

  (but then it won't be able to start any pods)

- For testing and development, having a single node is fine

- For production, make sure that you have extra capacity

  (so that your workload still fits if you lose a node or a group of nodes)

- Kubernetes is tested with [up to 5000 nodes](https://kubernetes.io/docs/setup/best-practices/cluster-large/)

  (however, running a cluster of that size requires a lot of tuning)

.debug[[k8s/concepts-k8s.md](file:///home/xeon/urgent/slides/k8s/concepts-k8s.md)]
---

class: extra-details

## Do we need to run Docker at all?

No!

--

- By default, Kubernetes uses the Docker Engine to run containers

- We can leverage other pluggable runtimes through the *Container Runtime Interface*

.debug[[k8s/concepts-k8s.md](file:///home/xeon/urgent/slides/k8s/concepts-k8s.md)]
---


## Interacting with Kubernetes

- We will interact with our Kubernetes cluster through the Kubernetes API

- The Kubernetes API is (mostly) RESTful

- It allows us to create, read, update, delete *resources*

- A few common resource types are:

  - node (a machine — physical or virtual — in our cluster)

  - pod (group of containers running together on a node)

  - service (stable network endpoint to connect to one or multiple containers)


.debug[[k8s/concepts-k8s.md](file:///home/xeon/urgent/slides/k8s/concepts-k8s.md)]
---

class: pic

.interstitial[![Image separating from the next module](https://gallant-turing-d0d520.netlify.com/containers/aerial-view-of-containers.jpg)]

---

name: toc-pods-and-containers-
class: title

 Pods and Containers 

.nav[
[Previous section](#toc-kubernetes-architecture)
|
[Back to table of contents](#toc-module-0)
|
[Next section](#toc-setting-up-kubernetes)
]

.debug[(automatically generated title slide)]

---

class: pic

# Pods and Containers 


![Pods](images/pods01.png)

.debug[[k8s/concepts-k8s.md](file:///home/xeon/urgent/slides/k8s/concepts-k8s.md)]
---

class: pic

![Pods](images/pods02.png)

.debug[[k8s/concepts-k8s.md](file:///home/xeon/urgent/slides/k8s/concepts-k8s.md)]
---

class: pic

![Pods](images/pods03.png)

.debug[[k8s/concepts-k8s.md](file:///home/xeon/urgent/slides/k8s/concepts-k8s.md)]
---

class: pic

![Pods](images/pods04.png)

.debug[[k8s/concepts-k8s.md](file:///home/xeon/urgent/slides/k8s/concepts-k8s.md)]
---

class: pic

![Pods](images/pods05.png)

.debug[[k8s/concepts-k8s.md](file:///home/xeon/urgent/slides/k8s/concepts-k8s.md)]
---


.debug[[k8s/concepts-k8s.md](file:///home/xeon/urgent/slides/k8s/concepts-k8s.md)]
---

class: pic

.interstitial[![Image separating from the next module](https://gallant-turing-d0d520.netlify.com/containers/blue-containers.jpg)]

---

name: toc-setting-up-kubernetes
class: title

 Setting up Kubernetes

.nav[
[Previous section](#toc-pods-and-containers-)
|
[Back to table of contents](#toc-module-0)
|
[Next section](#toc-kubectl)
]

.debug[(automatically generated title slide)]

---
# Setting up Kubernetes

- Kubernetes is made of many components that require careful configuration

- Secure operation typically requires TLS certificates and a local CA

  (certificate authority)

- Setting up everything manually is possible, but rarely done

  (except for learning purposes)

- Let's do a quick overview of available options!

.debug[[k8s/setup-overview.md](file:///home/xeon/urgent/slides/k8s/setup-overview.md)]
---

## Local development

- Examples: Docker Desktop, k3d, KinD, MicroK8s, Minikube

  (some of these also support clusters with multiple nodes)

.debug[[k8s/setup-overview.md](file:///home/xeon/urgent/slides/k8s/setup-overview.md)]
---

## Managed clusters

- Many cloud providers and hosting providers offer "managed Kubernetes"

- The deployment and maintenance of the cluster is entirely managed by the provider

  (ideally, clusters can be spun up automatically through an API, CLI, or web interface)

- Given the complexity of Kubernetes, this approach is *strongly recommended*

  (at least for your first production clusters)

- After working for a while with Kubernetes, you will be better equipped to decide:

  - whether to operate it yourself or use a managed offering

  - which offering or which distribution works best for you and your needs

.debug[[k8s/setup-overview.md](file:///home/xeon/urgent/slides/k8s/setup-overview.md)]
---

class: pic

![managed](images/managedhosts.png)

.debug[[k8s/setup-overview.md](file:///home/xeon/urgent/slides/k8s/setup-overview.md)]
---

## Kubernetes distributions and installers

- If you want to run Kubernetes yourselves, there are many options

  (free, commercial, proprietary, open source ...)

- Some of them are installers, while some are complete platforms

- Some of them leverage other well-known deployment tools

  (like Puppet, Terraform ...)

- A good starting point to explore these options is this [guide](https://v1-16.docs.kubernetes.io/docs/setup/#production-environment)

  (it defines categories like "managed", "turnkey" ...)

.debug[[k8s/setup-overview.md](file:///home/xeon/urgent/slides/k8s/setup-overview.md)]
---

## kubeadm

- kubeadm is a tool part of Kubernetes to facilitate cluster setup

- Many other installers and distributions use it (but not all of them)

- It can also be used by itself

- Excellent starting point to install Kubernetes on your own machines

  (virtual, physical, it doesn't matter)

.debug[[k8s/setup-overview.md](file:///home/xeon/urgent/slides/k8s/setup-overview.md)]
---

## Manual setup

- The resources below are mainly for educational purposes!

- [Kubernetes The Hard Way](https://github.com/kelseyhightower/kubernetes-the-hard-way) by Kelsey Hightower

  - step by step guide to install Kubernetes on Google Cloud

  - covers certificates, high availability ...

  - *“Kubernetes The Hard Way is optimized for learning, which means taking the long route to ensure you understand each task required to bootstrap a Kubernetes cluster.”*

- [Deep Dive into Kubernetes Internals for Builders and Operators](https://www.youtube.com/watch?v=3KtEAa7_duA)

  - conference presentation showing step-by-step control plane setup

  - emphasis on simplicity, not on security and availability

.debug[[k8s/setup-overview.md](file:///home/xeon/urgent/slides/k8s/setup-overview.md)]
---

## [Minikube](https://minikube.sigs.k8s.io/docs/)

- Supports many [drivers](https://minikube.sigs.k8s.io/docs/drivers/)

  (HyperKit, Hyper-V, KVM, VirtualBox, but also Docker and many others)

- Can deploy a single cluster; recent versions can deploy multiple nodes

- Great option if you want a "Kubernetes first" experience

.debug[[k8s/setup-overview.md](file:///home/xeon/urgent/slides/k8s/setup-overview.md)]
---

## Deploying a managed cluster

*"The easiest way to install Kubernetes is to get someone
else to do it for you."

- Let's see a few options to install managed clusters!

- This is not an exhaustive list

  (the goal is to show the actual steps to get started)

- The list is sorted alphabetically

- All the options mentioned here require an account
with a cloud provider

- ... And a credit card

.debug[[k8s/setup-overview.md](file:///home/xeon/urgent/slides/k8s/setup-overview.md)]
---

## AKS (initial setup)

- Install the Azure CLI

- Login:
  ```bash
  az login
  ```

- Select a [region](https://azure.microsoft.com/en-us/global-infrastructure/services/?products=kubernetes-service&regions=all
)

- Create a "resource group":
  ```bash
  az group create --name my-aks-group --location westeurope
  ```

.debug[[k8s/setup-overview.md](file:///home/xeon/urgent/slides/k8s/setup-overview.md)]
---
## AKS (create cluster)

- Create the cluster:
  ```bash
  az aks create --resource-group my-aks-group --name my-aks-cluster
  ```

- Wait about 5-10 minutes

- Add credentials to `kubeconfig`:
  ```bash
  az aks get-credentials --resource-group my-aks-group --name my-aks-cluster
  ```

.debug[[k8s/setup-overview.md](file:///home/xeon/urgent/slides/k8s/setup-overview.md)]
---

## AKS (cleanup)

- Delete the cluster:
  ```bash
  az aks delete --resource-group my-aks-group --name my-aks-cluster
  ```

- Delete the resource group:
  ```bash
  az group delete --resource-group my-aks-group
  ```

- Note: delete actions can take a while too!

  (5-10 minutes as well)

.debug[[k8s/setup-overview.md](file:///home/xeon/urgent/slides/k8s/setup-overview.md)]
---

## Amazon EKS (the old way)

- [Read the doc](https://docs.aws.amazon.com/eks/latest/userguide/getting-started-console.html)

- Create service roles, VPCs, and a bunch of other oddities

- Try to figure out why it doesn't work

- Start over, following an [official AWS blog post](https://aws.amazon.com/blogs/aws/amazon-eks-now-generally-available/)

- Try to find the missing Cloud Formation template

.debug[[k8s/setup-overview.md](file:///home/xeon/urgent/slides/k8s/setup-overview.md)]
---

## Amazon EKS (the new way)

- Install `eksctl`

- Set the usual environment variables

  ([AWS_DEFAULT_REGION](https://docs.aws.amazon.com/general/latest/gr/rande.html#eks_region), AWS_ACCESS_KEY, AWS_SECRET_ACCESS_KEY)

- Create the cluster:
  ```bash
  eksctl create cluster
  ```

- Cluster can take a long time to be ready (15-20 minutes is typical)

.debug[[k8s/setup-overview.md](file:///home/xeon/urgent/slides/k8s/setup-overview.md)]
---

## Amazon EKS (cleanup)

- Delete the cluster:
  ```bash
  eksctl delete cluster <clustername>
  ```

- If you need to find the name of the cluster:
  ```bash
  eksctl get clusters
  ```

.footnote[Note: the AWS documentation has been updated and now includes [eksctl instructions](https://docs.aws.amazon.com/eks/latest/userguide/getting-started-eksctl.html).]

.debug[[k8s/setup-overview.md](file:///home/xeon/urgent/slides/k8s/setup-overview.md)]
---

## Amazon EKS (notes)

- Convenient if you *have to* use AWS

- Needs extra steps to be truly production-ready

- Versions tend to be outdated

- The only officially supported pod network is the [Amazon VPC CNI plugin](https://docs.aws.amazon.com/eks/latest/userguide/pod-networking.html)

  - integrates tightly with security groups and VPC networking

  - not suitable for high density clusters (with many small pods on big nodes)

  - other plugins [should still work](https://docs.aws.amazon.com/eks/latest/userguide/alternate-cni-plugins.html) but will require extra work

.debug[[k8s/setup-overview.md](file:///home/xeon/urgent/slides/k8s/setup-overview.md)]
---

## GKE (initial setup)

- Install `gcloud`

- Login:
  ```bash
  gcloud auth init
  ```

- Create a "project":
  ```bash
  gcloud projects create my-gke-project
  gcloud config set project my-gke-project
  ```

- Pick a [region](https://cloud.google.com/compute/docs/regions-zones/)

  (example: `europe-west1`, `us-west1`, ...)

.debug[[k8s/setup-overview.md](file:///home/xeon/urgent/slides/k8s/setup-overview.md)]
---

## GKE (create cluster)

- Create the cluster:
  ```bash
  gcloud container clusters create my-gke-cluster --region us-west1 --num-nodes=2
  ```

  (without `--num-nodes` you might exhaust your IP address quota!)

- The first time you try to create a cluster in a given project, you get an error

  - you need to enable the Kubernetes Engine API
  - the error message gives you a link
  - follow the link and enable the API (and billing)
    <br/>(it's just a couple of clicks and it's instantaneous)

- Clutser should be ready in a couple of minutes

.debug[[k8s/setup-overview.md](file:///home/xeon/urgent/slides/k8s/setup-overview.md)]
---

## GKE (cleanup)

- List clusters (if you forgot its name):
  ```bash
  gcloud container clusters list
  ```

- Delete the cluster:
  ```bash
  gcloud container clusters delete my-gke-cluster --region us-west1
  ```

- Delete the project (optional):
  ```bash
  gcloud projects delete my-gke-project
  ```

.debug[[k8s/setup-overview.md](file:///home/xeon/urgent/slides/k8s/setup-overview.md)]
---

## GKE (notes)

- Well-rounded product overall

  (it used to be one of the best managed Kubernetes offerings available;
  now that many other providers entered the game, that title is debatable)

- The cluster comes with many add-ons

- Versions lag a bit:

  - latest minor version (e.g. 1.18) tends to be unsupported
 
  - previous minor version (e.g. 1.17) supported through alpha channel

  - previous versions (e.g. 1.14-1.16) supported

.debug[[k8s/setup-overview.md](file:///home/xeon/urgent/slides/k8s/setup-overview.md)]
---

class: pic

.interstitial[![Image separating from the next module](https://gallant-turing-d0d520.netlify.com/containers/chinook-helicopter-container.jpg)]

---

name: toc-kubectl
class: title

 Kubectl

.nav[
[Previous section](#toc-setting-up-kubernetes)
|
[Back to table of contents](#toc-module-0)
|
[Next section](#toc-deployments)
]

.debug[(automatically generated title slide)]

---
# Kubectl

- `kubectl` is (almost) the only tool we'll need to talk to Kubernetes

- It is a rich CLI tool around the Kubernetes API

  (Everything you can do with `kubectl`, you can do directly with the API)

- On our machines, there is a `~/.kube/config` file with:

  - the Kubernetes API address

  - the path to our TLS certificates used to authenticate

- You can also use the `--kubeconfig` flag to pass a config file

- Or directly `--server`, `--user`, etc.

- `kubectl` can be pronounced "Cube C T L", "Cube cuttle", "Cube cuddle"...

.debug[[k8s/kubectlget.md](file:///home/xeon/urgent/slides/k8s/kubectlget.md)]
---

class: extra-details

## `kubectl` is the new SSH

- We often start managing servers with SSH

  (installing packages, troubleshooting ...)

- At scale, it becomes tedious, repetitive, error-prone

- Instead, we use config management, central logging, etc.

- In many cases, we still need SSH:

  - as the underlying access method (e.g. Ansible)

  - to debug tricky scenarios

  - to inspect and poke at things

.debug[[k8s/kubectlget.md](file:///home/xeon/urgent/slides/k8s/kubectlget.md)]
---

class: extra-details

## The parallel with `kubectl`

- We often start managing Kubernetes clusters with `kubectl`

  (deploying applications, troubleshooting ...)

- At scale (with many applications or clusters), it becomes tedious, repetitive, error-prone

- Instead, we use automated pipelines, observability tooling, etc.

- In many cases, we still need `kubectl`:

  - to debug tricky scenarios

  - to inspect and poke at things

- The Kubernetes API is always the underlying access method

.debug[[k8s/kubectlget.md](file:///home/xeon/urgent/slides/k8s/kubectlget.md)]
---

class: pic

![kubectl](images/kubectlget.png)

.debug[[k8s/kubectlget.md](file:///home/xeon/urgent/slides/k8s/kubectlget.md)]
---

class: pic

![kubectl](images/kubectlget02.png)

.debug[[k8s/kubectlget.md](file:///home/xeon/urgent/slides/k8s/kubectlget.md)]
---

class: pic

![kubectl](images/kubectlget3.png)

.debug[[k8s/kubectlget.md](file:///home/xeon/urgent/slides/k8s/kubectlget.md)]
---

## `kubectl get`

- Let's look at our `Node` resources with `kubectl get`!

.exercise[

- Look at the composition of our cluster:
  ```bash
  kubectl get node
  ```

- These commands are equivalent:
  ```bash
  kubectl get no
  kubectl get node
  kubectl get nodes
  ```

]

.debug[[k8s/kubectlget.md](file:///home/xeon/urgent/slides/k8s/kubectlget.md)]
---

## Obtaining machine-readable output

- `kubectl get` can output JSON, YAML, or be directly formatted

.exercise[

- Give us more info about the nodes:
  ```bash
  kubectl get nodes -o wide
  ```

- Let's have some YAML:
  ```bash
  kubectl get no -o yaml
  ```
  See that `kind: List` at the end? It's the type of our result!

]

.debug[[k8s/kubectlget.md](file:///home/xeon/urgent/slides/k8s/kubectlget.md)]
---

## (Ab)using `kubectl` and `jq`

- It's super easy to build custom reports

.exercise[

- Show the capacity of all our nodes as a stream of JSON objects:
  ```bash
    kubectl get nodes -o json |
            jq ".items[] | {name:.metadata.name} + .status.capacity"
  ```

]

.debug[[k8s/kubectlget.md](file:///home/xeon/urgent/slides/k8s/kubectlget.md)]
---

class: extra-details

## Exploring types and definitions

- We can list all available resource types by running `kubectl api-resources`
  <br/>
  (In Kubernetes 1.10 and prior, this command used to be `kubectl get`)

- We can view the definition for a resource type with:
  ```bash
  kubectl explain type
  ```

- We can view the definition of a field in a resource, for instance:
  ```bash
  kubectl explain node.spec
  ```

- Or get the full definition of all fields and sub-fields:
  ```bash
  kubectl explain node --recursive
  ```

.debug[[k8s/kubectlget.md](file:///home/xeon/urgent/slides/k8s/kubectlget.md)]
---

class: extra-details

## Introspection vs. documentation

- We can access the same information by reading the [API documentation](https://kubernetes.io/docs/reference/#api-reference)

- The API documentation is usually easier to read, but:

  - it won't show custom types (like Custom Resource Definitions)

  - we need to make sure that we look at the correct version

- `kubectl api-resources` and `kubectl explain` perform *introspection*

  (they communicate with the API server and obtain the exact type definitions)

.debug[[k8s/kubectlget.md](file:///home/xeon/urgent/slides/k8s/kubectlget.md)]
---

## Type names

- The most common resource names have three forms:

  - singular (e.g. `node`, `service`, `deployment`)

  - plural (e.g. `nodes`, `services`, `deployments`)

  - short (e.g. `no`, `svc`, `deploy`)

- Some resources do not have a short name

- `Endpoints` only have a plural form

  (because even a single `Endpoints` resource is actually a list of endpoints)

.debug[[k8s/kubectlget.md](file:///home/xeon/urgent/slides/k8s/kubectlget.md)]
---

## Viewing details

- We can use `kubectl get -o yaml` to see all available details

- However, YAML output is often simultaneously too much and not enough

- For instance, `kubectl get node node1 -o yaml` is:

  - too much information (e.g.: list of images available on this node)

  - not enough information (e.g.: doesn't show pods running on this node)

  - difficult to read for a human operator

- For a comprehensive overview, we can use `kubectl describe` instead

.debug[[k8s/kubectlget.md](file:///home/xeon/urgent/slides/k8s/kubectlget.md)]
---

## `kubectl describe`

- `kubectl describe` needs a resource type and (optionally) a resource name

- It is possible to provide a resource name *prefix*

  (all matching objects will be displayed)

- `kubectl describe` will retrieve some extra information about the resource

.exercise[

- Look at the information available for `node1` with one of the following commands:
  ```bash
  kubectl describe node/node1
  kubectl describe node node1
  ```

]

(We should notice a bunch of control plane pods.)

.debug[[k8s/kubectlget.md](file:///home/xeon/urgent/slides/k8s/kubectlget.md)]
---

## Listing running containers

- Containers are manipulated through *pods*

- A pod is a group of containers:

 - running together (on the same node)

 - sharing resources (RAM, CPU; but also network, volumes)

.exercise[

- List pods on our cluster:
  ```bash
  kubectl get pods
  ```

]

--

*Where are the pods that we saw just a moment earlier?!?*

.debug[[k8s/kubectlget.md](file:///home/xeon/urgent/slides/k8s/kubectlget.md)]
---

## Namespaces

- Namespaces allow us to segregate resources

.exercise[

- List the namespaces on our cluster with one of these commands:
  ```bash
  kubectl get namespaces
  kubectl get namespace
  kubectl get ns
  ```

]

.debug[[k8s/kubectlget.md](file:///home/xeon/urgent/slides/k8s/kubectlget.md)]
---

## Accessing namespaces

- By default, `kubectl` uses the `default` namespace

- We can see resources in all namespaces with `--all-namespaces`

.exercise[

- List the pods in all namespaces:
  ```bash
  kubectl get pods --all-namespaces
  ```

- Since Kubernetes 1.14, we can also use `-A` as a shorter version:
  ```bash
  kubectl get pods -A
  ```

]

.debug[[k8s/kubectlget.md](file:///home/xeon/urgent/slides/k8s/kubectlget.md)]
---

## What are all these control plane pods?

- `etcd` is our etcd server

- `kube-apiserver` is the API server

- `kube-controller-manager` and `kube-scheduler` are other control plane components

- `coredns` provides DNS-based service discovery 

- `kube-proxy` is the (per-node) component managing port mappings and such

- the `READY` column indicates the number of containers in each pod

.debug[[k8s/kubectlget.md](file:///home/xeon/urgent/slides/k8s/kubectlget.md)]
---

## Scoping another namespace

- We can also look at a different namespace (other than `default`)

.exercise[

- List only the pods in the `kube-system` namespace:
  ```bash
  kubectl get pods --namespace=kube-system
  kubectl get pods -n kube-system
  ```

]

.debug[[k8s/kubectlget.md](file:///home/xeon/urgent/slides/k8s/kubectlget.md)]
---

## Namespaces and other `kubectl` commands

- We can use `-n`/`--namespace` with almost every `kubectl` command

- Example:

  - `kubectl create --namespace=X` to create something in namespace X

- We can use `-A`/`--all-namespaces` with most commands that manipulate multiple objects


- `kubectl delete` can delete resources across multiple namespaces

- `kubectl label` can add/remove/update labels across multiple namespaces

.debug[[k8s/kubectlget.md](file:///home/xeon/urgent/slides/k8s/kubectlget.md)]
---

class: extra-details

## What about `kube-public`?

.exercise[

- List the pods in the `kube-public` namespace:
  ```bash
  kubectl -n kube-public get pods
  ```

]

Nothing!

`kube-public` is created by kubeadm & [used for security bootstrapping](https://kubernetes.io/blog/2017/01/stronger-foundation-for-creating-and-managing-kubernetes-clusters).

.debug[[k8s/kubectlget.md](file:///home/xeon/urgent/slides/k8s/kubectlget.md)]
---

class: extra-details

## Exploring `kube-public`

- The only interesting object in `kube-public` is a ConfigMap named `cluster-info`

.exercise[

- List ConfigMap objects:
  ```bash
  kubectl -n kube-public get configmaps
  ```

- Inspect `cluster-info`:
  ```bash
  kubectl -n kube-public get configmap cluster-info -o yaml
  ```

]

Note the `selfLink` URI: `/api/v1/namespaces/kube-public/configmaps/cluster-info`

We can use that!

.debug[[k8s/kubectlget.md](file:///home/xeon/urgent/slides/k8s/kubectlget.md)]
---

class: extra-details

## Accessing `cluster-info`

- Earlier, when trying to access the API server, we got a `Forbidden` message

- But `cluster-info` is readable by everyone (even without authentication)

.exercise[

- Retrieve `cluster-info`:
  ```bash
  curl -k https://10.96.0.1/api/v1/namespaces/kube-public/configmaps/cluster-info
  ```

]

- We were able to access `cluster-info` (without auth)

- It contains a `kubeconfig` file

.debug[[k8s/kubectlget.md](file:///home/xeon/urgent/slides/k8s/kubectlget.md)]
---

class: extra-details

## Retrieving `kubeconfig`

- We can easily extract the `kubeconfig` file from this ConfigMap

.exercise[

- Display the content of `kubeconfig`:
  ```bash
    curl -sk https://10.96.0.1/api/v1/namespaces/kube-public/configmaps/cluster-info \
         | jq -r .data.kubeconfig
  ```

]

- This file holds the canonical address of the API server, and the public key of the CA

- This file *does not* hold client keys or tokens

- This is not sensitive information, but allows us to establish trust

.debug[[k8s/kubectlget.md](file:///home/xeon/urgent/slides/k8s/kubectlget.md)]
---

class: extra-details

## What about `kube-node-lease`?

- Starting with Kubernetes 1.14, there is a `kube-node-lease` namespace

  (or in Kubernetes 1.13 if the NodeLease feature gate is enabled)

- That namespace contains one Lease object per node

- *Node leases* are a new way to implement node heartbeats

  (i.e. node regularly pinging the control plane to say "I'm alive!")

- For more details, see [KEP-0009] or the [node controller documentation]

[KEP-0009]: https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/0009-node-heartbeat.md
[node controller documentation]: https://kubernetes.io/docs/concepts/architecture/nodes/#node-controller


  

.debug[[k8s/kubectlget.md](file:///home/xeon/urgent/slides/k8s/kubectlget.md)]
---

class: pic

.interstitial[![Image separating from the next module](https://gallant-turing-d0d520.netlify.com/containers/container-cranes.jpg)]

---

name: toc-deployments
class: title

 Deployments

.nav[
[Previous section](#toc-kubectl)
|
[Back to table of contents](#toc-module-0)
|
[Next section](#toc-labels-and-annotations)
]

.debug[(automatically generated title slide)]

---
# Deployments

- Remember, we can't create containers directly

- The desired state is described in a Deployment YAML file

- A deployment creates Replicaset which creates Pods

- The Deployment is a high level controller

.debug[[k8s/kubectl-run.md](file:///home/xeon/urgent/slides/k8s/kubectl-run.md)]
---

## Pod

- Can have one or multiple containers

- Runs on a single node

  (Pod cannot "straddle" multiple nodes)

- Pods cannot be moved

  (e.g. in case of node outage)

- Pods cannot be scaled

  (except by manually creating more Pods)

.debug[[k8s/kubectl-run.md](file:///home/xeon/urgent/slides/k8s/kubectl-run.md)]
---

class: extra-details

## Pod details

- A Pod is not a process; it's an environment for containers

  - it cannot be "restarted"

  - it cannot "crash"

- The containers in a Pod can crash

- They may or may not get restarted

  (depending on Pod's restart policy)

- If all containers exit successfully, the Pod ends in "Succeeded" phase

- If some containers fail and don't get restarted, the Pod ends in "Failed" phase

.debug[[k8s/kubectl-run.md](file:///home/xeon/urgent/slides/k8s/kubectl-run.md)]
---

## Replica Set

- Set of identical (replicated) Pods

- Defined by a pod template + number of desired replicas


.debug[[k8s/kubectl-run.md](file:///home/xeon/urgent/slides/k8s/kubectl-run.md)]
---

## `kubectl run` 

- From Kubernetes 1.18, `kubectl run` creates a Pod

  - other kinds of resources can still be created with `kubectl create`

.debug[[k8s/kubectl-run.md](file:///home/xeon/urgent/slides/k8s/kubectl-run.md)]
---

## Creating a Deployment 

- Let's destroy that `pingpong` app that we created

- Then we will use `kubectl create deployment` to re-create it

.debug[[k8s/kubectl-run.md](file:///home/xeon/urgent/slides/k8s/kubectl-run.md)]
---

class: extra-details

## In Kubernetes 1.19

- Since Kubernetes 1.19, we can specify the command to run

- The command must be passed after two dashes:
  ```bash
  kubectl create deployment pingpong --image=alpine -- ping 127.1
  ```

.debug[[k8s/kubectl-run.md](file:///home/xeon/urgent/slides/k8s/kubectl-run.md)]
---

## Viewing container output

- Let's use the `kubectl logs` command

- We will pass either a *pod name*, or a *type/name*


.debug[[k8s/kubectl-run.md](file:///home/xeon/urgent/slides/k8s/kubectl-run.md)]
---

## Streaming logs in real time

- Just like `docker logs`, `kubectl logs` supports convenient options:

  - `-f`/`--follow` to stream logs in real time (à la `tail -f`)

  - `--tail` to indicate how many lines you want to see (from the end)

  - `--since` to get logs only after a given timestamp

.debug[[k8s/kubectl-run.md](file:///home/xeon/urgent/slides/k8s/kubectl-run.md)]
---

## Scaling our application

- We can create additional copies of our container (I mean, our pod) with `kubectl scale`

.exercise[

- Scale our `pingpong` deployment:
  ```bash
  kubectl scale deploy/pingpong --replicas 3
  ```

- Note that this command does exactly the same thing:
  ```bash
  kubectl scale deployment pingpong --replicas 3
  ```

- Check that we now have multiple pods:
  ```bash
  kubectl get pods
  ```

]

.debug[[k8s/kubectl-run.md](file:///home/xeon/urgent/slides/k8s/kubectl-run.md)]
---

class: extra-details

## Scaling a Replica Set

- What if we scale the Replica Set instead of the Deployment?

- The Deployment would notice it right away and scale back to the initial level

- The Replica Set makes sure that we have the right numbers of Pods

- The Deployment makes sure that the Replica Set has the right size

  (conceptually, it delegates the management of the Pods to the Replica Set)

- This might seem weird (why this extra layer?) but will soon make sense

  (when we will look at how rolling updates work!)

.debug[[k8s/kubectl-run.md](file:///home/xeon/urgent/slides/k8s/kubectl-run.md)]
---

## Lifecycle

- What happens when a Pod gets created?

--

- Pod phases:

--

  - Pending

--

  - Running

--

  - Succeeded

--

  - Failed

--

  - Unknown

--

  - CrashLoopBackOff

---
.debug[[k8s/kubectl-run.md](file:///home/xeon/urgent/slides/k8s/kubectl-run.md)]
---
## 19,000 words

They say, "a picture is worth one thousand words."

The following 19 slides show what really happens when we run:

```bash
kubectl create deployment web --image=nginx
```

.debug[[k8s/deploymentslideshow.md](file:///home/xeon/urgent/slides/k8s/deploymentslideshow.md)]
---
class: pic
![](images/kubectl-create-deployment-slideshow/01.svg)
.debug[[k8s/deploymentslideshow.md](file:///home/xeon/urgent/slides/k8s/deploymentslideshow.md)]
---
class: pic
![](images/kubectl-create-deployment-slideshow/02.svg)
.debug[[k8s/deploymentslideshow.md](file:///home/xeon/urgent/slides/k8s/deploymentslideshow.md)]
---
class: pic
![](images/kubectl-create-deployment-slideshow/03.svg)
.debug[[k8s/deploymentslideshow.md](file:///home/xeon/urgent/slides/k8s/deploymentslideshow.md)]
---
class: pic
![](images/kubectl-create-deployment-slideshow/04.svg)
.debug[[k8s/deploymentslideshow.md](file:///home/xeon/urgent/slides/k8s/deploymentslideshow.md)]
---
class: pic
![](images/kubectl-create-deployment-slideshow/05.svg)
.debug[[k8s/deploymentslideshow.md](file:///home/xeon/urgent/slides/k8s/deploymentslideshow.md)]
---
class: pic
![](images/kubectl-create-deployment-slideshow/06.svg)
.debug[[k8s/deploymentslideshow.md](file:///home/xeon/urgent/slides/k8s/deploymentslideshow.md)]
---
class: pic
![](images/kubectl-create-deployment-slideshow/07.svg)
.debug[[k8s/deploymentslideshow.md](file:///home/xeon/urgent/slides/k8s/deploymentslideshow.md)]
---
class: pic
![](images/kubectl-create-deployment-slideshow/08.svg)
.debug[[k8s/deploymentslideshow.md](file:///home/xeon/urgent/slides/k8s/deploymentslideshow.md)]
---
class: pic
![](images/kubectl-create-deployment-slideshow/09.svg)
.debug[[k8s/deploymentslideshow.md](file:///home/xeon/urgent/slides/k8s/deploymentslideshow.md)]
---
class: pic
![](images/kubectl-create-deployment-slideshow/10.svg)
.debug[[k8s/deploymentslideshow.md](file:///home/xeon/urgent/slides/k8s/deploymentslideshow.md)]
---
class: pic
![](images/kubectl-create-deployment-slideshow/11.svg)
.debug[[k8s/deploymentslideshow.md](file:///home/xeon/urgent/slides/k8s/deploymentslideshow.md)]
---
class: pic
![](images/kubectl-create-deployment-slideshow/12.svg)
.debug[[k8s/deploymentslideshow.md](file:///home/xeon/urgent/slides/k8s/deploymentslideshow.md)]
---
class: pic
![](images/kubectl-create-deployment-slideshow/13.svg)
.debug[[k8s/deploymentslideshow.md](file:///home/xeon/urgent/slides/k8s/deploymentslideshow.md)]
---
class: pic
![](images/kubectl-create-deployment-slideshow/14.svg)
.debug[[k8s/deploymentslideshow.md](file:///home/xeon/urgent/slides/k8s/deploymentslideshow.md)]
---
class: pic
![](images/kubectl-create-deployment-slideshow/15.svg)
.debug[[k8s/deploymentslideshow.md](file:///home/xeon/urgent/slides/k8s/deploymentslideshow.md)]
---
class: pic
![](images/kubectl-create-deployment-slideshow/16.svg)
.debug[[k8s/deploymentslideshow.md](file:///home/xeon/urgent/slides/k8s/deploymentslideshow.md)]
---
class: pic
![](images/kubectl-create-deployment-slideshow/17.svg)
.debug[[k8s/deploymentslideshow.md](file:///home/xeon/urgent/slides/k8s/deploymentslideshow.md)]
---
class: pic
![](images/kubectl-create-deployment-slideshow/18.svg)
.debug[[k8s/deploymentslideshow.md](file:///home/xeon/urgent/slides/k8s/deploymentslideshow.md)]
---
class: pic
![](images/kubectl-create-deployment-slideshow/19.svg)

.debug[[k8s/deploymentslideshow.md](file:///home/xeon/urgent/slides/k8s/deploymentslideshow.md)]
---
## Declarative vs Imperative

- With Kubernetes, we cannot say: "run this container"

- All we can do is write a *spec* and push it to the API server

- The API server will validate that spec 

- Then it will store it in etcd

- A *controller* will "notice" that spec and act upon it

.debug[[k8s/declarative.md](file:///home/xeon/urgent/slides/k8s/declarative.md)]
---

## Deploying with YAML

- So far, we created resources with the following commands:

  - `kubectl run`

  - `kubectl create deployment`

- We can also create resources directly with YAML manifests

.debug[[k8s/declarative.md](file:///home/xeon/urgent/slides/k8s/declarative.md)]
---

## `kubectl apply` vs `create`

- `kubectl create -f whatever.yaml`

  - creates resources if they don't exist

  - if resources already exist, don't alter them
    <br/>(and display error message)

- `kubectl apply -f whatever.yaml`

  - creates resources if they don't exist

  - if resources already exist, update them
    <br/>(to match the definition provided by the YAML file)

  - stores the manifest as an *annotation* in the resource

.debug[[k8s/declarative.md](file:///home/xeon/urgent/slides/k8s/declarative.md)]
---

## Creating multiple resources

- The manifest can contain multiple resources separated by `---`

```yaml
 kind: ...
 apiVersion: ...
 metadata: ...
   name: ...
 ...
 ---
 kind: ...
 apiVersion: ...
 metadata: ...
   name: ...
 ...
```
.debug[[k8s/declarative.md](file:///home/xeon/urgent/slides/k8s/declarative.md)]
---

## Deleting resources

- We can also use a YAML file to *delete* resources

- `kubectl delete -f ...` will delete all the resources mentioned in a YAML file

  (useful to clean up everything that was created by `kubectl apply -f ...`)

- The definitions of the resources don't matter

  (just their `kind`, `apiVersion`, and `name`)




.debug[[k8s/declarative.md](file:///home/xeon/urgent/slides/k8s/declarative.md)]
---

class: pic

.interstitial[![Image separating from the next module](https://gallant-turing-d0d520.netlify.com/containers/container-housing.jpg)]

---

name: toc-labels-and-annotations
class: title

 Labels and Annotations

.nav[
[Previous section](#toc-deployments)
|
[Back to table of contents](#toc-module-0)
|
[Next section](#toc-volumes)
]

.debug[(automatically generated title slide)]

---
# Labels and Annotations

- Most Kubernetes resources can have *labels* and *annotations*

- Both labels and annotations are arbitrary strings

  (with some limitations that we'll explain in a minute)

- Both labels and annotations can be added, removed, changed, dynamically

- This can be done with:

  - the `kubectl edit` command

  - the `kubectl label` and `kubectl annotate`

  - ... many other ways! (`kubectl apply -f`, `kubectl patch`, ...)

.debug[[k8s/labels-annotations.md](file:///home/xeon/urgent/slides/k8s/labels-annotations.md)]
---

## Viewing labels and annotations

- Let's see what we get when we create a Deployment

.exercise[

- Create a Deployment:
  ```bash
  kubectl create deployment clock --image=ahmedgabercod/clock
  ```

- Look at its annotations and labels:
  ```bash
  kubectl describe deployment clock
  ```

]

So, what do we get?

.debug[[k8s/labels-annotations.md](file:///home/xeon/urgent/slides/k8s/labels-annotations.md)]
---

## Labels and annotations for our Deployment

- We see one label:
  ```
  Labels: app=clock
  ```

- This is added by `kubectl create deployment`

- And one annotation:
  ```
  Annotations: deployment.kubernetes.io/revision: 1
  ```

- This is to keep track of successive versions when doing rolling updates

.debug[[k8s/labels-annotations.md](file:///home/xeon/urgent/slides/k8s/labels-annotations.md)]
---

## And for the related Pod?

- Let's look up the Pod that was created and check it too

.exercise[

- Find the name of the Pod:
  ```bash
  kubectl get pods
  ```

- Display its information:
  ```bash
  kubectl describe pod clock-xxxxxxxxxx-yyyyy
  ```

]

So, what do we get?

.debug[[k8s/labels-annotations.md](file:///home/xeon/urgent/slides/k8s/labels-annotations.md)]
---

## Labels and annotations for our Pod

- We see two labels:
  ```
    Labels: app=clock
            pod-template-hash=xxxxxxxxxx
  ```

- `app=clock` comes from `kubectl create deployment` too

- `pod-template-hash` was assigned by the Replica Set

  (when we will do rolling updates, each set of Pods will have a different hash)

- There are no annotations:
  ```
  Annotations: <none>
  ```

.debug[[k8s/labels-annotations.md](file:///home/xeon/urgent/slides/k8s/labels-annotations.md)]
---

## Selectors

- A *selector* is an expression matching labels

- It will restrict a command to the objects matching *at least* all these labels

.exercise[

- List all the pods with at least `app=clock`:
  ```bash
  kubectl get pods --selector=app=clock
  ```

- List all the pods with a label `app`, regardless of its value:
  ```bash
  kubectl get pods --selector=app
  ```

]

.debug[[k8s/labels-annotations.md](file:///home/xeon/urgent/slides/k8s/labels-annotations.md)]
---

## Settings labels and annotations

- The easiest method is to use `kubectl label` and `kubectl annotate`

.exercise[

- Set a label on the `clock` Deployment:
  ```bash
  kubectl label deployment clock color=blue
  ```

- Check it out:
  ```bash
  kubectl describe deployment clock
  ```

]

.debug[[k8s/labels-annotations.md](file:///home/xeon/urgent/slides/k8s/labels-annotations.md)]
---

## Other ways to view labels

- `kubectl get` gives us a couple of useful flags to check labels

- `kubectl get --show-labels` shows all labels

- `kubectl get -L xyz` shows the value of label `xyz`

.exercise[

- List all the labels that we have on pods:
  ```bash
  kubectl get pods --show-labels
  ```

- List the value of label `app` on these pods:
  ```bash
  kubectl get pods -L app
  ```

]

.debug[[k8s/labels-annotations.md](file:///home/xeon/urgent/slides/k8s/labels-annotations.md)]
---

class: extra-details

## More on selectors

- If a selector has multiple labels, it means "match at least these labels"

  Example: `--selector=app=frontend,release=prod`

- `--selector` can be abbreviated as `-l` (for **l**abels)

  We can also use negative selectors

  Example: `--selector=app!=clock`

- Selectors can be used with most `kubectl` commands

  Examples: `kubectl delete`, `kubectl label`, ...


.debug[[k8s/labels-annotations.md](file:///home/xeon/urgent/slides/k8s/labels-annotations.md)]
---

class: pic

.interstitial[![Image separating from the next module](https://gallant-turing-d0d520.netlify.com/containers/containers-by-the-water.jpg)]

---

name: toc-volumes
class: title

 Volumes

.nav[
[Previous section](#toc-labels-and-annotations)
|
[Back to table of contents](#toc-module-0)
|
[Next section](#toc-configmaps--secrets)
]

.debug[(automatically generated title slide)]

---
# Volumes

- Volumes are special directories that are mounted in containers

- Volumes can have many different purposes:

  - share files and directories between containers running on the same machine

  - share files and directories between containers and their host

.debug[[k8s/volumes.md](file:///home/xeon/urgent/slides/k8s/volumes.md)]
---

class: pic

![pervolumes](images/pervolumes.png)

.debug[[k8s/volumes.md](file:///home/xeon/urgent/slides/k8s/volumes.md)]
---

## Adding a volume

- Volumes are added in two places:

  - at the Pod level (to declare the volume)

  - at the container level (to mount the volume)

.debug[[k8s/volumes.md](file:///home/xeon/urgent/slides/k8s/volumes.md)]
---

## Our basic Pod

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-without-volume
spec:
  containers:
  - name: nginx
    image: nginx
```

.debug[[k8s/volumes.md](file:///home/xeon/urgent/slides/k8s/volumes.md)]
---

## The Pod with a volume

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-with-volume
spec:
  volumes:
  - name: www
  containers:
  - name: nginx
    image: nginx
    volumeMounts:
    - name: www
      mountPath: /usr/share/nginx/html/
```
.debug[[k8s/volumes.md](file:///home/xeon/urgent/slides/k8s/volumes.md)]
---

## Sharing a volume between two containers

.small[
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-with-git
spec:
  volumes:
  - name: www
  containers:
  - name: nginx
    image: nginx
    volumeMounts:
    - name: www
      mountPath: /usr/share/nginx/html/
  - name: git
    image: alpine
    command: [ "sh", "-c", "echo "Hello" > /www/index.html" ]
    volumeMounts:
    - name: www
      mountPath: /www/
  restartPolicy: OnFailure
```
]

.debug[[k8s/volumes.md](file:///home/xeon/urgent/slides/k8s/volumes.md)]
---

## Volume lifecycle

- The lifecycle of a volume is linked to the pod's lifecycle

- This means that a volume is created when the pod is created

- This is mostly relevant for `emptyDir` volumes

  (other volumes, like remote storage, are not "created" but rather "attached" )

- A volume survives across container restarts

- A volume is destroyed (or, for remote storage, detached) when the pod is destroyed

.debug[[k8s/volumes.md](file:///home/xeon/urgent/slides/k8s/volumes.md)]
---

## Persistent Volumes Claims

- Our Pods can use a special volume type: a *Persistent Volume Claim*

- A Persistent Volume Claim (PVC) is also a Kubernetes resource

  (visible with `kubectl get persistentvolumeclaims` or `kubectl get pvc`)

- A PVC is not a volume; it is a *request for a volume*

- It should indicate at least:

  - the size of the volume (e.g. "5 GiB")

  - the access mode (e.g. "read-write by a single pod")

.debug[[k8s/volumes.md](file:///home/xeon/urgent/slides/k8s/volumes.md)]
---

## What's in a PVC?

- A PVC contains at least:

  - a list of *access modes* (ReadWriteOnce, ReadOnlyMany, ReadWriteMany)

  - a size (interpreted as the minimal storage space needed)

- It can also contain optional elements:

  - a selector (to restrict which actual volumes it can use)

  - a *storage class* (used by dynamic provisioning, more on that later)

.debug[[k8s/volumes.md](file:///home/xeon/urgent/slides/k8s/volumes.md)]
---

## What does a PVC look like?

Here is a manifest for a basic PVC:

```yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
   name: my-claim
spec:
   accessModes:
     - ReadWriteOnce
   resources:
     requests:
       storage: 1Gi
```
.debug[[k8s/volumes.md](file:///home/xeon/urgent/slides/k8s/volumes.md)]
---

## Using a Persistent Volume Claim

Here is a Pod definition like the ones shown earlier, but using a PVC:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-using-a-claim
spec:
  containers:
  - image: ...
    name: container-using-a-claim
    volumeMounts:
    - mountPath: /my-vol
      name: my-volume
  volumes:
  - name: my-volume
    persistentVolumeClaim:
      claimName: my-claim
```
.debug[[k8s/volumes.md](file:///home/xeon/urgent/slides/k8s/volumes.md)]
---

class: pic

.interstitial[![Image separating from the next module](https://gallant-turing-d0d520.netlify.com/containers/distillery-containers.jpg)]

---

name: toc-configmaps--secrets
class: title

 Configmaps | Secrets

.nav[
[Previous section](#toc-volumes)
|
[Back to table of contents](#toc-module-0)
|
[Next section](#toc-)
]

.debug[(automatically generated title slide)]

---
# Configmaps | Secrets

- Some applications need to be configured 

- There are many ways for our code to pick up configuration:

  - command-line arguments

  - environment variables

  - configuration files

  - configuration servers 

- How can we do these things with containers and Kubernetes?

.debug[[k8s/configuration.md](file:///home/xeon/urgent/slides/k8s/configuration.md)]
---

## Passing configuration to containers

- There are many ways to pass configuration to code running in a container:

  - command-line arguments

  - environment variables

  - injecting configuration files



.debug[[k8s/configuration.md](file:///home/xeon/urgent/slides/k8s/configuration.md)]
---

## Environment variables, pros and cons

- Works great when the running program expects these variables

- Works great for optional parameters with reasonable defaults

.debug[[k8s/configuration.md](file:///home/xeon/urgent/slides/k8s/configuration.md)]
---

## Injecting configuration files

- Sometimes, there is no way around it: we need to inject a full config file

- Kubernetes provides a mechanism for that purpose: `configmaps`

- A configmap is a Kubernetes resource that exists in a namespace

- Conceptually, it's a key/value map

  (values are arbitrary strings)

- We can think about them in two different ways:

  - as holding entire configuration file(s)

  - as holding individual configuration parameters

.debug[[k8s/configuration.md](file:///home/xeon/urgent/slides/k8s/configuration.md)]
---

## Configmaps storing entire files

- In this case, each key/value pair corresponds to a configuration file

- Key = name of the file

- Value = content of the file

.debug[[k8s/configuration.md](file:///home/xeon/urgent/slides/k8s/configuration.md)]
---

## Configmaps storing individual parameters

- In this case, each key/value pair corresponds to a parameter

- Key = name of the parameter

- Value = value of the parameter


.debug[[k8s/configuration.md](file:///home/xeon/urgent/slides/k8s/configuration.md)]
---

## Exposing configmaps to containers

- Configmaps can be exposed as plain files in the filesystem of a container


- Configmaps can be exposed as environment variables in the container


.debug[[k8s/configuration.md](file:///home/xeon/urgent/slides/k8s/configuration.md)]
---

## Passwords, tokens, sensitive information

- For sensitive information, there is another special resource: *Secrets*

- Secrets and Configmaps work almost the same way

.debug[[k8s/configuration.md](file:///home/xeon/urgent/slides/k8s/configuration.md)]
---
## Differences between configmaps and secrets
 
- Secrets are base64-encoded when shown with `kubectl get secrets -o yaml`

  - keep in mind that this is just *encoding*, not *encryption*

  - it is very easy to extract and decode secrets
.debug[[k8s/configuration.md](file:///home/xeon/urgent/slides/k8s/configuration.md)]
---
## Exercise —

- Create a configMap called 'options' with the value server=local 
- Create a json file with the following content:
```
{
   "username": "YOUR_NAME",
   "password": "PASSWORD",
   "email": "YOUR_EMAIL"
}
```
   - Save the file as secret.json
   - Create a generic secret object from the file
   - Extract the password value from the secret and store it in password.txt

- Validate your resources

.debug[[k8s/exercise-configmap.md](file:///home/xeon/urgent/slides/k8s/exercise-configmap.md)]
---
class: title, in-person

Thank you! <br/> Questions?

.debug[[shared/thankyou.md](file:///home/xeon/urgent/slides/shared/thankyou.md)]</textarea>
    <script src="remark.min.js" type="text/javascript">
    </script>
    <script type="text/javascript">
      var slideshow = remark.create({
        ratio: '16:9',
        highlightSpans: true,
        slideNumberFormat: '%current%/%total%',
        excludedClasses: ["self-paced","snap","btp-auto","benchmarking","elk-manual","prom-manual","extra-details"]
      });
    </script>
    
    <!-- 
    These two scripts will be available only when loading the
    content using the pub/sub server. Otherwise, they'll just
    404 and that's OK.
    -->
    <script src="/socket.io/socket.io.js">
    </script>
    <script src="/remote.js">
    </script>

  </body>
</html>
